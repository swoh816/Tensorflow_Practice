{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MNIST dataset\n",
    "\n",
    "In this tutorial, we will use `MNIST dataset` to make an algorithm that can distinguish numbers between 1~10.\n",
    "First, we need to download `MNIST dataset.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are 70,000 data points (i.e., images) in total: 55,000 training data points (mnist.train), 10,000 test data (mnist.test), and 5,000 validation data (mnist.validation).\n",
    "\n",
    "A MNIST data point has two parts: an image of handwritten digit (`mnist.X.images`), and a corresponding label (`mnist.X.labels`). X can be either `train`, `test`, or `validation`. Each image is `28x28` pixels, which is a array of 784 numbers (i.e., 784-dimensional vector space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABrCAYAAABnlHmpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb81065be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(mnist.train.images[0].reshape([28,28]), cmap=plt.get_cmap(\"Greys\"))\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(mnist.train.images[1].reshape([28,28]), cmap=plt.get_cmap(\"Greys\"))\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(mnist.train.images[2].reshape([28,28]), cmap = plt.get_cmap(\"Greys\"))\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(mnist.train.images[3].reshape([28,28]), cmap = plt.get_cmap(\"Greys\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Step-by-step tutorial for building a model for number recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set a certain dimensional variable without specific values (i.e., *placeholder*), you use `placeholder.` **It is a value that we'll input when we ask `tensorflow` to run a computation.** To see the benefits of using `placeholder`, see https://www.tensorflow.org/get_started/mnist/beginners.\n",
    "\n",
    "Here, we create a variable with zero row and 784 columns (784 is the number of features for MNIST dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `None` means that a row dimsenion can be of any length.\n",
    "\n",
    "Next, we use `Variable` as a modifiable tensor that lives in TensorFlow's graph of interacting operations. For machine learning applications, one generally has the **model parameters** be Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W` has a shape of [784, 10] because it has 784 pixels (features) and 10 numbers (1~10). So, the output of `W*x + b` will be the probability of correspond to the numbers between 1~10.\n",
    "\n",
    "We use `tf.zeros` to fill `W` and `b` with zeros, but because `W` and `b` are what we are going to learn and thus change, the initial values don't matter much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_man = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement cross entropy, we need a placeholder for the correct answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can implement the cross-entropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y_man), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write `cross_entropy` from Tensorflow, and then you can rewrite above two lines as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tf = tf.matmul(x, W) + b\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_tf, labels=y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the difference between `y_man` and `y_tf` is that **`y_man` has `tf.softmax` applied to the regression function `tf.matmul(x,W)+b`, while `y_tf` only has the regression function.** The reason is because `tf.nn.softmax_cross_entropy_with_logits` automatically calculates the softmax activation function applied to the model's prediction (y) to operate cross-entropy between y and the target (y\\_), the model's prediction `y`\n",
    "__should not__ have `softmax` in itself, while when we write cross-entropy function manually, we need to apply softmax activation function to `y` manually as well.\n",
    "\n",
    "Now, you use `Gradient Descent` to calculate (1) the steepest descent of the loss with respect to each variables, with (2) step length 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "### The above is a short for:\n",
    "# optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "# train_step = optimizer.minimize(cross_entropy).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict = {x:batch_xs, y_:batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: you need to use `y_` for `feed_dict`, not `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_man, 1), tf.argmax(y_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9152\n"
     ]
    }
   ],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Side-by-side comparison between cross entropy from library and manual script\n",
    "\n",
    "***Note*** <br>\n",
    "Use of cross-entropy from tensorflow library: <https://www.tensorflow.org/get_started/mnist/pros> <br>\n",
    "Making cross-entropy algorithm manually: <https://www.tensorflow.org/get_started/mnist/beginners>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9173\n"
     ]
    }
   ],
   "source": [
    "#Manual cross entropy\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "cross_entropy_man = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy_man)\n",
    "tf.global_variables_initializer().run()\n",
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9186\n"
     ]
    }
   ],
   "source": [
    "#Cross entropy from tensorflow library\n",
    "y = tf.matmul(x, W) + b\n",
    "cross_entropy_lib = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy_lib)\n",
    "tf.global_variables_initializer().run()\n",
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT NOTE: EASY MISTAKES TO MAKE\n",
    "\n",
    "If you mistakenly apply `softmax` function to `y` while you use `soft_max_cross_entropy`, then you'll get much smaller accuracy rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9069\n"
     ]
    }
   ],
   "source": [
    "# Wrong version\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "cross_entropy_lib = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy_lib)\n",
    "tf.global_variables_initializer().run()\n",
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
